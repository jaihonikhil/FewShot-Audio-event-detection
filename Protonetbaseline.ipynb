{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Protonetbaseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnGvEH4v77yx"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import ndimage\n",
        "import multiprocessing as mp\n",
        "import cv2\n",
        "from tqdm import tqdm_notebook\n",
        "from tqdm import tnrange\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn \n",
        "import torchvision\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNHl3ze2Fm5i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0be06bd2-472c-404a-922e-625a22782460"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06JMnEbd70MK"
      },
      "source": [
        "**LOADING THE DATA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF5evKjoR1at"
      },
      "source": [
        "file=\"/content/drive/MyDrive/spectrograms\"\n",
        "files_in_directory = os.listdir(file) #EDIT THIS FOR ANOTHER DIRECTORY\n",
        "examples = [file for file in files_in_directory]\n",
        "\n",
        "print(examples)\n",
        "XXtrain=[]\n",
        "YYtrain=[]\n",
        "\n",
        "for e in range(len(examples)-3):\n",
        "  path=file+\"/\"+\"class_\"+str(e)\n",
        "  f=os.listdir(path)\n",
        "  for foo in f:\n",
        "    jk=np.array(Image.open(path+\"/\"+foo))\n",
        "    XXtrain.append(jk)\n",
        "    YYtrain.append(e)\n",
        "lxtrain=np.array(XXtrain)\n",
        "lytrain=np.array(YYtrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6gmQ-vh7Fiw"
      },
      "source": [
        "file=\"/content/drive/MyDrive/spectrograms/val\"\n",
        "files_in_directory = os.listdir(file)#EDIT THIS FOR ANOTHER DIRECTORY\n",
        "examples = [file for file in files_in_directory]\n",
        "XXtest=[]\n",
        "YYtest=[]\n",
        "for e in range(len(examples)):\n",
        "  path=file+\"/\"+\"class_\"+str(e+6)\n",
        "  f=os.listdir(path)\n",
        "  for foo in f:\n",
        "    jk=np.array(Image.open(path+\"/\"+foo))\n",
        "    XXtest.append(jk)\n",
        "    YYtest.append(e+6)\n",
        "lxtest=np.array(XXtest)\n",
        "lytest=np.array(YYtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yBook8_IzVL"
      },
      "source": [
        "np.save('/content/drive/MyDrive/lxtrain', lxtrain)\n",
        "np.save('/content/drive/MyDrive/lytrain', lytrain)\n",
        "np.save('/content/drive/MyDrive/lxtest', lxtest)\n",
        "np.save('/content/drive/MyDrive/lytest', lytest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTI2UQ4oiLfZ"
      },
      "source": [
        "lxtrain.shape, lytrain.shape, lxtest.shape, lytest.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Says4f1079oa"
      },
      "source": [
        "**PROTOTYPICAL NETWORKS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgiiF93bBTRj"
      },
      "source": [
        "def euclidean_dist(x, y):\n",
        "  a = x.size(0)\n",
        "  b = y.size(0)\n",
        "  c = x.size(1)\n",
        "  x = x.unsqueeze(1).expand(a, b, c)\n",
        "  y = y.unsqueeze(0).expand(a, b, c)\n",
        "\n",
        "  return torch.pow(x - y, 2).sum(2)\n",
        "  #for finding the euclidean distance between the prototype and the query"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fwWGx2sd7Lv"
      },
      "source": [
        "class Flattening(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Flattening, self).__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x.view(x.size(0), -1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSPN-ZtbESY0"
      },
      "source": [
        "def Protonet_conv2d(xshape, hiddenshape, yshape):\n",
        "#protonet emdeddings and the main model\n",
        "  embed = torch.nn.Sequential(\n",
        "    torch.nn.Conv2d(xshape[0], hiddenshape, 3, padding=1),\n",
        "    torch.nn.BatchNorm2d(hiddenshape),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.MaxPool2d(3),\n",
        "    torch.nn.Conv2d(hiddenshape, hiddenshape, 3, padding=1),\n",
        "    torch.nn.BatchNorm2d(hiddenshape),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.MaxPool2d(3),\n",
        "    torch.nn.Conv2d(hiddenshape, hiddenshape, 3, padding=1),\n",
        "    torch.nn.BatchNorm2d(hiddenshape),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.MaxPool2d(3),\n",
        "    torch.nn.Conv2d(hiddenshape, yshape, 3, padding=1),\n",
        "    torch.nn.BatchNorm2d(yshape),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.MaxPool2d(3),\n",
        "    Flattening()\n",
        "    )\n",
        "    \n",
        "  return load_samples(embed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8K8xIii-kVL"
      },
      "source": [
        "class load_samples(torch.nn.Module):\n",
        "  def __init__(self, embed):\n",
        "    \n",
        "    super(load_samples, self).__init__()\n",
        "    self.embed = embed.cuda()\n",
        "#for predicting loss, accuracy and prediction  \n",
        "  def loss_and_pred(self, spectograms):\n",
        "    specs = spectograms['specs'].cuda()\n",
        "    n_way = spectograms['n_way']\n",
        "    n_support = spectograms['support']\n",
        "    n_query = spectograms['query']\n",
        "    support_data = specs[:, :support]\n",
        "    labels = torch.arange(0, n_way).view(n_way, 1, 1).expand(n_way, query, 1).long()\n",
        "    query_data = specs[:, support:]\n",
        "    labels = Variable(labels, requires_grad=False)\n",
        "    # print(labels)\n",
        "    labels = labels.cuda()\n",
        "    x = torch.cat([support_data.contiguous().view(n_way * support, *support_data.size()[2:]),query_data.contiguous().view(n_way * n_query, *query_data.size()[2:])], 0)\n",
        "    y = self.embed.forward(x)\n",
        "    # print(y.size)\n",
        "    yshape = y.size(-1) \n",
        "    query_y = y[n_way*support:]\n",
        "    prototype = y[:n_way*support].view(n_way, support, yshape).mean(1)\n",
        "    # print(\"prototype\")\n",
        "    # print(prototype)\n",
        "    distance = euclidean_dist(query_y, prototype)\n",
        "    #finding the probability and the distance between prototype and the query\n",
        "    probab = F.log_softmax(-distance,dim=1).view(n_way, query, -1)\n",
        "    print(probab)\n",
        "    lossprobab = -probab.gather(2, labels).squeeze().view(-1).mean()\n",
        "    m, yk = probab.max(2)\n",
        "    accuracy = torch.eq(yk, labels.squeeze()).float().mean()\n",
        "    print(accuracy)\n",
        "    return lossprobab, {'loss': lossprobab.item(),'acc': accuracy.item(),'yk': yk}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj5wSa1F8F56"
      },
      "source": [
        "**EPISODIC TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TFGLISJEhh8"
      },
      "source": [
        "def train(model, optimizer, train_x, train_y, n_way, support, query, max_epoch, epoch_size):\n",
        "  \n",
        "  #divide the learning rate by 2 at each epoch, as suggested in paper\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5, last_epoch=-1)\n",
        "  epoch = 0 #epochs done so far\n",
        "  #here n_way =5 and training is done in episodic manner\n",
        "  while epoch < max_epoch:\n",
        "    loss_total = 0.0\n",
        "    accuracy = 0.0\n",
        "\n",
        "    for episode in tnrange(epoch_size, desc=\"Epoch {:d} train\".format(epoch+1)):\n",
        "      batch = []\n",
        "      K = np.random.choice(np.unique(train_y), n_way)\n",
        "      # print(K)\n",
        "      for cls in K:\n",
        "        # print(cls)\n",
        "        datax_cls = train_x[train_y == cls]\n",
        "        perm = np.random.permutation(datax_cls)\n",
        "        batch_cls = perm[:(support+query)]\n",
        "        # print(batch_cls)\n",
        "        batch.append(batch_cls)\n",
        "      batch = np.array(batch)\n",
        "      batch = torch.from_numpy(batch).float()\n",
        "      batch = batch.permute(0,1,4,2,3)\n",
        "      subset = {'specs': batch,'n_way': n_way,'support': support,'query': query}\n",
        "      optimizer.zero_grad()\n",
        "      loss, output = model.loss_and_pred(subset)\n",
        "      # print(loss)\n",
        "      loss_total += output['loss']\n",
        "      accuracy += output['acc']\n",
        "      # print(accuracy)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    epoch_loss = loss_total / epoch_size\n",
        "    # print(epoch_size)\n",
        "    epoch_acc = accuracy / epoch_size\n",
        "    print('Epoch {:d} -- Loss: {:.4f} Acc: {:.4f}'.format(epoch+1,epoch_loss, epoch_acc))\n",
        "    epoch += 1\n",
        "    # print(epoch)\n",
        "    scheduler.step()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dwDJULIBCv0"
      },
      "source": [
        "#for loading the dataset from the drive\n",
        "lxtrain = np.load('/content/drive/MyDrive/urbansound8k/lxtrain.npy')\n",
        "lytrain = np.load('/content/drive/MyDrive/urbansound8k/lytrain.npy')\n",
        "lxtest = np.load('/content/drive/MyDrive/urbansound8k/lxtest.npy')\n",
        "lytest = np.load('/content/drive/MyDrive/urbansound8k/lytest.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSmsQERZ-uvL"
      },
      "source": [
        "model = Protonet_conv2d(xshape=(4, 120, 128),hiddenshape=128,yshape=128)\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.003)\n",
        "#adam optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdpttHA2Fpnw"
      },
      "source": [
        "filename=\"/content/drive/MyDrive/urbansound8k/bestmodel.pt\"\n",
        "n_way = 5 # number of classes taken at a time\n",
        "support = 5 # number of examples in support set\n",
        "query = 5 # number of examples in query set\n",
        "max_epoch = 10\n",
        "epoch_size = 2000\n",
        "train(model, optimizer, lxtrain, lytrain, n_way, support, query, max_epoch, epoch_size)\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
        "#for saving the best model to be used in future\n",
        "torch.save(model,filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQDkXHAWQAiR"
      },
      "source": [
        "model = torch.load('/content/drive/MyDrive/urbansound8k/bestmodel.pt')\n",
        "#loading the saved model use only if the model is saved else dont use this command"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-HE3eb78LYy"
      },
      "source": [
        "**TESTING AND EVALUATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcjCPCm3KeVo"
      },
      "source": [
        "##TESTTINGGG\n",
        "def test(model, test_x, test_y, n_way, n_support, n_query, test_episode):\n",
        "  accuracy = 0.0\n",
        "  loss_total = 0.0\n",
        "  for episode in tnrange(test_episode):\n",
        "    batch = []\n",
        "    K = np.random.choice(np.unique(test_y), n_way)\n",
        "    # print(K)\n",
        "    for cls in K:\n",
        "      # print(cls)\n",
        "      datax_cls = test_x[test_y == cls]\n",
        "      perm = np.random.permutation(datax_cls)\n",
        "      batch_cls = perm[:(support+query)]\n",
        "      batch.append(batch_cls)\n",
        "      # episodic training in progress and in batchwise\n",
        "    batch = np.array(batch)\n",
        "    # print(batch)\n",
        "    batch = torch.from_numpy(batch).float()\n",
        "    batch = batch.permute(0,1,4,2,3)\n",
        "    subset = {'specs': batch,'n_way': n_way,'support': n_support,'query': n_query}\n",
        "    loss, output = model.loss_and_pred(subset)\n",
        "    # print(output)\n",
        "    loss_total += output['loss']\n",
        "    accuracy += output['acc']\n",
        "  avg_loss = loss_total / test_episode\n",
        "  avg_acc = accuracy / test_episode\n",
        "  print('Test results -- Loss: {:.4f} Acc: {:.4f}'.format(avg_loss, avg_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ttqqMYtHWqR"
      },
      "source": [
        "EVALUATION METRICS: The average accuracy calculated over the test episodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vj0SPYvAKkHQ"
      },
      "source": [
        "n_way = 5 # number of class selection at random\n",
        "support = 5\n",
        "query = 10 \n",
        "test_x = lxtest\n",
        "test_y = lytest\n",
        "test_episode = 2000\n",
        "test(model, test_x, test_y, n_way, support, query, test_episode)\n",
        "#this command calculated the prediction and weigted accuracy across each episodes and averages them same goes for the loss we can get the evaluation and accuracy either from this or generated a predicted csv file which is later taken as input by the evaluation file to calculate the accuracya and the F1 Score."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OEOBe-sHS2Z"
      },
      "source": [
        " **CODE ENDS**"
      ]
    }
  ]
}